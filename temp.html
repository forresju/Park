<!doctype html>

<html lang="en">

<head>
    <meta charset="utf-8">
    <title>Test</title>
    <meta name="description" content="Test">
    <style>
        body {
            font-family: sans-serif;
            margin: 0 auto;
            width: 1024px;
            line-height: 1.5;
        }

        img {
            height: 480px;
        }

        pre {
            background-color: whitesmoke;
        }
    </style>
</head>

<body>
<h1>Park!</h1>
<div style="text-align: center;">
<img src="README_images/park_demo.gif" alt="Welcome to Park!" style="width:100%;" />
</div>
<h2>Table of Contents</h2>
<ul>
<li><a href="#p1" title="Introduction">Introduction</a></li>
<li><a href="#p2" title="Setting Up the Development Environment">Setting Up the Development Environment</a></li>
<li><a href="#p3" title="Adding the Machine Learning Packages">Adding the Machine Learning Packages</a></li>
<li><a href="#p4" title="Running the Scripts">Running the Scripts</a></li>
<ul>
<li><a href="#d1" title="OpenCV Image Capture and Display">OpenCV Image Capture and Display</a></li>
<li><a href="#d2" title="Mask R-CNN Object Detection">Mask R-CNN Object Detection</a></li>
<li><a href="#d3" title="Recognizing and Counting Vehicles">Recognizing and Counting Vehicles</a></li>
<li><a href="#d4" title="Creating and Displaying zones">Creating and Displaying Zones</a></li>
<li><a href="#d5" title="Counting Vehicles in Zones">Counting Vehicles in Zones</a></li>
<li><a href="#d6" title="Creating the Database">Creating the Database</a></li>
<li><a href="#d7" title="El Super Demo">El Super Demo</a></li>
</ul>
<li><a href="#p5" title="Schedule the Scripts Using cron">Schedule the Scripts Using cron</a></li>
<li><a href="#p10" title="References">References</a>
</ul>
<br>
<hr>
<h2 id="p1">Introduction</h2>
<p>As interns at NASA Langley, aka "The Sherpas", we worked on a lot of projects. Some of them involved machine learning, and, yes, this included a parking lot project.</p>
<p>The use case was that finding parking is a problem for everyone. In 2016, drivers in New York City spent an average of 107 hours and $2243 a year looking for parking (INRIX, 2017). In addition, there are over 120 outstanding requests for proposals for city and institutional parking management (IPMI, 2019). One of the original Sherpas, who had been working on a similar project at Georgia Tech, suggested creating a parking management system for NASA Langley's Digital Transformation initiative, and it was approved.</p>
<p>We looked at various solutions, including sensors in each parking space; Light Detection and Ranging (LIDAR) to count cars entering and exiting parking areas; etc. Each approach had issues that made it impractical to implement with the resources we had; for example, most image recognition systems we looked at required a high, almost overhead, angle of view.</p>
<p>Finally, we came across <a href="https://medium.com/@ageitgey/snagging-parking-spaces-with-mask-r-cnnand-python-955f2231c400" target="_blank" title="Snagging parking spaces with Mask R-CNN and Python">Adam Geitgey's awesome article on object detection</a>, which introduced us to <a href="https://github.com/matterport/Mask_RCNN" target="_blank" title="Matterport Mask R-CNN">Matterport Mask R-CNN</a> (Geitgey, 2019). We realized that his implementation of Mask R-CNN was something we could use with the resources available to us, especially the lower and more practical viewing angle. However, Adam’s solution, designed for a single system for a single user looking at a single parking lot, was too expensive in terms of processing power and time. The first time we implemented it, it sent our cooling fans into overdrive and hung our computers for several minutes; run another of our projects, <a href="http://acronymsfortina.rgprogramming.com/index.html" target="_blank" title="Acronyms for Tina">Acronyms for Tina</a>, on localhost for a similar, albeit less intense, experience.</p>
<p>Still, it worked, so using what we learned, we went back to the drawing board to develop a system that would rotate through multiple lots; analyze and collect parking data, and present this information to users in a timely manner. After much trial and error, we came up with a system that leveraged Linux scripting, Python, OpenCV, TensorFlow, SQL, and PHP to:</p>
<ol>
<li>Use a cron task scheduler to access the cameras overlooking each lot in turn.</li>
<li>Capture a frame from each camera; divide it into zones (e.g., employee, handicap, etc.); and count the number of vehicles in each zone.</li>
<li>Aggregate the results and collect them in a database.</li>
<li>Have the front-end pull data for users from the database.</li>
</ol>
<p>Due to architecture, security requirements, cross-lot tracking, etc., the NASA application was a bit complex, and for those same reasons, we will not recreate it here. However, here are a series of demos, written in Python, that breakdown the way the machine learning aspect works. Besides a practical application of TensorFlow, Mask R-CNN, and computer vision, this is also a very good introduction to the Common Objects in COntext (COCO) dataset. Have fun and good luck!</p>
<hr>
<h2 id="p2">Setting Up the Development Environment</h2>
<p>As we just said, we'll be using CentOS Linux 7 in VirtualBox for this demo, but you can use another virtual machine or an actual server if you like. Just make sure that your system has at least 2GB of memory; 16GB of hard disk space; 128MB of video memory; and a connection to the Internet.</p>
<div style="background-color: lightgoldenrodyellow; padding: 6px;">
<p>
<b>NOTE</b> - If you do choose to use CentOS or Red Hat Linux, we've included another README file name CENTOS with directions on how to set up your environment.</p>
</div>
<br>
<div style="background-color: lightgoldenrodyellow; padding: 6px;">
<p>
<b>NOTE</b> - While the production code for NASA only generated data, the demo scripts also display images. To view these images, you will need an X Server. For these demos, our solution was to use the GNOME Graphical User Interface (GUI) for development; it comes with a display server; it's lightweight; and it allows you to cut-and-paste from the host machine into the VM's Terminal. To use GNOME on CentOS or Red Hat Linux, use the following commands:</p>
<pre>
sudo yum -y groupinstall "GNOME Desktop"
sudo startx
</pre>
</div>
<p>Our first step is to make sure the operating system is up to date. On CentOS, we would use the following command; other flavors of Linux may use "sudo apt-get update" instead, while Windows users can run "wuauclt.exe /updatenow":</p>
<pre>
[park@localhost ~]# sudo yum -y update
</pre>
<p>This may take a while, especially on a new system.</p>
<p>Once the system update is completed, make sure that the tools needed for development are installed:</p>
<ol>
<li><b>Python 3 Programming Language Interpreter and PIP Python Package Installer</b> - While Python 2 is installed with CentOS by default, we will need Python 3 to run our computer vision and machine learning scripts, specifically Python 3.6.x. There are a few ways of doing this, but we will use the IUS Community Repo; for an in-depth look at options, check out <a href="https://www.hogarthuk.com/?q=node/15" title="Running newer applications on CentOS" target="_blank">this link from James Hogarth</a>. To install Python, run the following command: 
<pre>
[park@localhost ~]# sudo yum -y install https://centos7.iuscommunity.org/ius-release.rpm
[park@localhost ~]# sudo yum -y install python36u
[park@localhost ~]# sudo yum -y install python36u-pip
[park@localhost ~]# sudo yum -y install python36u-devel
[park@localhost ~]# python3 --version
[park@localhost ~]# pip3 --version
</pre>
</li>
<li><b>SQLite RDBMS</b> - For portability (and to keep this README as short as possible), we'll be using SQLite. Check the version using the following command: 
<pre>
[park@localhost ~]# sqlite3 -version
3.7.17 2013-05-20 00:56:22 118a3b35693b134d56ebd780123b7fd6f1497668
</pre>
<p>If SQLite is not installed, install it using the following command:</p>
<pre>
[park@localhost ~]# sudo yum -y install sqlite
</pre>
</li>
<li><b>cron Time-Based Job Scheduler</b> - cron should already be installed by default, but check anyway:
<pre>
[park@localhost ~]# whereis -b crontab | cut -d' ' -f2 | xargs rpm -qf
cronie-1.4.11-19.e17.x86_64
</pre>
<p>If cron is not installed, install it using the following command:</p>
<pre>
[park@localhost ~]# yum -y install cronie
</pre>
</li>
</ol>
<p>Alright! Before continuing, let's do another update of the system using the following command:</p>
<pre>
[park@localhost ~]# sudo yum -y update
</pre>
<p>Just in case, we'll double check everything is installed and updated using the following commands:</p>
<pre>
[park@localhost ~]# python3 ––version
[park@localhost ~]# pip3 ––version
[park@localhost ~]# sqlite3 -version
[park@localhost ~]# whereis -b crontab | cut -d' ' -f2 | xargs rpm -qf
</pre>
<figure style="text-align: center;">
<img style="height: 480px" src="README_images/readme01.png" alt="Verifying initial setup" />
<figcaption style="margin-top: 0; font-style: italic; font-size: smaller;">Verifying initial setup </figcaption>
</figure>
</div>
<hr>
<h2 id="p3">Adding the Machine Learning Packages</h2>
<p>Our next step is to add the packages we will need to run the scripts.</p>
<div style="background-color: lightgoldenrodyellow; padding: 6px;">
<p>
<b>NOTE</b> - If you like, you can clone this repository into your folder: <strong>Just make sure you do so before installing the ML packages.</strong> Use the following command to clone the repo:</p>
<pre>
git clone https://github.com/garciart/Park.git
</pre>
<p>This will create a folder "Park" with all the code in the right place. If you are using a shared folder, fetch into your shared folder instead of cloning:</p>
<pre>
[park@localhost ~]# cd Park # replace Park with the name of your shared folder
[park@localhost Park]$ git init
[park@localhost Park]$ git remote add origin https://github.com/garciart/Park.git
[park@localhost Park]$ git fetch
[park@localhost Park]$ git checkout origin/master -ft
</pre>
<p>As a rule, we don't use the "Master" branch, just like we don't use "root" for development. You don't have to, but we suggest creating a separate branch:</p>
<pre>
[park@localhost ~]# cd Park # replace Park with the name of your shared folder
[park@localhost Park]$ git branch park # replace park with your username
[park@localhost Park]$ git checkout park # replace park with your username
[park@localhost Park]$ git status
</pre>
<p>In addition, remember to use "Park" as your development folder, not the home folder (in our case, "park"). </p>
</div>
<figure style="text-align: center;">
<img style="height: 480px" src="README_images/readme02.png" alt="Cloning the Park repository" />
<figcaption style="margin-top: 0; font-style: italic; font-size: smaller;">Cloning the Park repository </figcaption>
</figure>
</div>
<p>Clone the excellent <a href="https://github.com/matterport/Mask_RCNN" target="_blank" title="Mask R-CNN engine from Matterport">Mask R-CNN engine from Matterport</a>, which will serve as our object detection and instance segmentation engine using the following commands:</p>
<pre>
[park@localhost Park]$ git clone https://github.com/matterport/Mask_RCNN.git
[park@localhost Park]$ cd Mask_RCNN
[park@localhost Mask_RCNN]$ ls
[park@localhost Mask_RCNN]$ cat requirements.txt
</pre>
<figure style="text-align: center;">
<img style="height: 480px" src="README_images/readme03.png" alt="Cloning the Mask R-CNN repository" />
<figcaption style="margin-top: 0; font-style: italic; font-size: smaller;">Cloning the Mask R-CNN repository </figcaption>
</figure>
</div>
<p>Note the setup.py file, which will build and install the Mask RCNN engine, and the requirements.txt file, which contains the list of modules and packages we will need to run the Mask RCNN engine:</p>
<ul>
<li><a href="https://numpy.org/" title="numpy" target="_blank">numpy</a> - support for high-level mathematics involving arrays and matrices.</li>
<li><a href="https://scipy.org/scipylib" title="scipy" target="_blank">scipy</a> - support for scientific and technical computing.</li>
<li><a href="https://python-pillow.org/" title="Pillow" target="_blank">Pillow</a> - Python Imaging Library to open, manipulate and save image files.</li>
<li><a href="https://cython.org/" title="cython" target="_blank">cython</a> - support for C extensions and datatypes in Python.</li>
<li><a href="https://matplotlib.org/" title="matplotlib" target="_blank">matplotlib</a> - support for 2d plotting.</li>
<li><a href="https://scikit-image.org/" title="scikit-image" target="_blank">scikit-image</a> - support for image processing.</li>
<li><a href="https://www.tensorflow.org/" title="tensorflow" target="_blank">tensorflow>=1.3.0</a> - support for machine learning and tensor mathematics.</li>
<li><a href="https://keras.io/" title="keras" target="_blank">keras>=2.0.8</a> - support for neural networks.</li>
<li><a href="https://opencv.org/" title="opencv" target="_blank">opencv-python</a> - support for real-time computer vision.</li>
<li><a href="https://www.h5py.org/" title="h5py" target="_blank">h5py</a> - support for implementing HDF5 binary data format.</li>
<li><a href="https://imgaug.readthedocs.io/en/latest/" title="imgaug" target="_blank">imgaug</a> - support for for image augmentation in machine learning.</li>
<li><a href="https://ipython.org/" title="IPython" target="_blank">IPython[all]</a> - support for interactive computing.</li>
</ul>
<p>However, before executing installing requirements.txt, we have to make some changes. Unfortunately, Matterport's version of Mask R-CNN cannot use TensorFlow 2.0 or Keras 2.3 or above. In addition, we will need to install <a href="https://docs.opencv.org/master/" title="opencv-python-contrib" target="_blank">OpenCV's extra modules</a> to display images. Using your favorite editor, change the TensorFlow and Keras lines to the following:</p>
<pre>
tensorflow&gt;=1.3.0,&lt;2.0
keras&gt;=2.0.8,&lt;2.3
</pre>
<p>In addition, add the following line after the OpenCV line:</p>
<pre>
opencv-contrib-python
</pre>
<figure style="text-align: center;">
<img style="height: 480px" src="README_images/readme04.png" alt="Editing requirements.txt" />
<figcaption style="margin-top: 0; font-style: italic; font-size: smaller;">Editing requirements.txt </figcaption>
</figure>
</div>
<p>We also have to make sure our <a href="https://setuptools.readthedocs.io/en/latest/" title="Setuptools" target="_blank">Setuptools</a> are up-to-date. Otherwise, we may run into errors extracting and creating modules and packages from requirements.txt. In addition, we have to make sure <a href="https://docs.python.org/3/library/tkinter.html" title="tkinter" target="_blank">tkinter, the standard Python interface to the Tk GUI toolkit</a>, is installed as well, or you may get a "matplotlib is currently using a non-GUI backend" error. Therefore, enter the following commands: </p>
<pre>
[park@localhost Mask_RCNN]$ pip3 install --upgrade setuptools --user
[park@localhost Mask_RCNN]$ sudo yum -y install python3-tkinter
</pre>
<p>Once the installation is complete, install Mask R-CNN's requirements using the following command:</p>
<pre>
[park@localhost Mask_RCNN]$ pip3 install -r requirements.txt --user
</pre>
<figure style="text-align: center;">
<img style="height: 480px" src="README_images/readme05.png" alt="Installing Mask R-CNN requirements" />
<figcaption style="margin-top: 0; font-style: italic; font-size: smaller;">Installing Mask R-CNN requirements</figcaption>
</figure>
</div>
<p>Hopefully, everything went well; if not, check the verbose comments for any errors and correct them. Remember, you may have to use a different flavor of a package depending on the system you are using. For example, we used Red Hat Linux, so for some packages, we had to use their version (e.g., python36-mysql, etc.). You can look for packages using:</p>
<pre>
[park@localhost Mask_RCNN]$ pip3 list
- or -
[park@localhost Mask_RCNN]$ yum search [package name]
</pre>
<p>Once everything is set, run the setup script using the command:</p>
<pre>
[park@localhost Mask_RCNN]$ python3 setup.py install --user
</pre>
<figure style="text-align: center;">
<img style="height: 480px" src="README_images/readme06.png" alt="Setting up Mask R-CNN" />
<figcaption style="margin-top: 0; font-style: italic; font-size: smaller;">Setting up Mask R-CNN </figcaption>
</figure>
</div>
<p>Once again, check the verbose comments for any errors and correct them. Finally, go back one directory and download the Common Objects in Context (COCO) dataset by using the following commands:</p>
<pre>
[park@localhost Mask_RCNN]$ cd ..
[park@localhost Park]$ wget https://github.com/matterport/Mask_RCNN/releases/download/v2.0/mask_rcnn_coco.h5
</pre>
<p>The COCO dataset consists of over 100,000 images of 80 objects, including cars (#3), buses (#6), and trucks (#8); find out more at <a href="http://cocodataset.org" title="COCO Dataset" target="_blank">http://cocodataset.org</a>. Please note that you can replace this dataset with a more specific set for vehicle detection, but for now, we will use the COCO dataset: </p>
<figure style="text-align: center;">
<img style="height: 480px" src="README_images/readme07.png" alt="Getting the COCO dataset" />
<figcaption style="margin-top: 0; font-style: italic; font-size: smaller;">Getting the COCO dataset </figcaption>
</figure>
</div>
<hr>
<h2 id="p4">Running the Scripts</h2>
<p>Now comes the fun part: Our Python code is a combination of Matterport's open-source Mask-RCNN's samples (Matterport, 2019), Alex Geitgey's excellent article on detecting open parking spaces (Geitgey, 2019), and our own embellishments (all MIT licensed, of course). By the time you are done working through these demos, you should have a good understanding how the heavy lifting occurs on the back end.</p>
<div style="background-color: lightgoldenrodyellow; padding: 6px;">
<p>
<b>NOTE</b> - To view these images generated by the demo scripts, you will need an X Server; otherwise, OpenCV will display a "Failed to Start the X server" error. Our solution was to use the GNOME Graphical User Interface (GUI) for development; it came with a display server; it was lightweight; and it allowed us to cut-and-paste from the host machine into the VM's Terminal. To do so on Linux, use the following commands:</p>
<pre>
sudo yum -y groupinstall "GNOME Desktop"
sudo startx
</pre>
</div>
<p>Download the demos to your development folder if you did not clone or fetch the repo. There are seven demos, and each one adds new functionality to Park.</p>
<ul>
<li>demo1.py - OpenCV image capture and display demo.</li>
<li>demo2.py - Adds Mask R-CNN object detection to the demo.</li>
<li>demo3.py - Adds recognizing and counting vehicles to the demo.</li>
<li>demo4.py - Adds creating and displaying zones to the demo.</li>
<li>demo5.py - Adds counting vehicles in zones to the demo.</li>
<li>demo6.py - Creates the database.</li>
<li>demo7.py - El Super Demo: Incorporates communicating with the database and the functionality of all of the previous demos.</li>
</ul>
<p>You will also need the log, capture, image, and video folders. If you cloned or fetched the repository, your directory should look like this:</p>
<figure style="text-align: center;">
<img style="height: 480px" src="README_images/readme08.png" alt="Development folder listing" />
<figcaption style="margin-top: 0; font-style: italic; font-size: smaller;">Development folder listing </figcaption>
</figure>
</div>
<p>Once you are all set up, open the code for each demo in your favorite editor as we get to it and follow along. By the way, each script has its own "shebang", so you can run them directly by setting their permissions to executable.</p>
<hr>
<h3 id="d1">Demo 1 - OpenCV Image Capture and Display</h3>
<p>This one is pretty easy, but using OpenCV is the foundation of Park's image recognition system. Note that we listed four FRAME_SOURCE's (Lines 16-20), with three of them commented out:</p>
<pre>
# Image, video or camera to process - set this to 0 to use your webcam instead of a file
# FRAME_SOURCE = [(IMAGE_DIR + "/demo_image1.jpg"),(IMAGE_DIR + "/demo_image2.jpg"),(IMAGE_DIR + "/demo_image3.jpg")]
# FRAME_SOURCE = [(VIDEO_DIR + "/demo_video1.mp4"),(VIDEO_DIR + "/demo_video2.mp4"),(VIDEO_DIR + "/demo_video3.mp4")]
# FRAME_SOURCE = [(IMAGE_DIR + "/demo_image.jpg")]
FRAME_SOURCE = ["https://raw.githubusercontent.com/garciart/Park/master/demo_images/demo_image.jpg"]
</pre>
<p>Park can work with images, videos and live streams. While we use demo_image.jpg, feel free to switch sources during the first three demos. However, to work correctly with this tutorial, Demos 4 through 5 need to use demo_image.jpg, while Demo 7 pulls what it needs from the database you will create in Demo 6. </p>
<p>Note the colors. OpenCV uses a Blue-Green-Red (BGR) color model, which we converted to an RGB color model for Mask R-CNN (Line 34):</p>
<pre>
# Convert the image from BGR color (which OpenCV uses) to RGB color
rgb_image = frame[:, :, ::-1]
</pre>
<p>We did not notice a difference in detection accuracy when we ran the other scripts, but we do what the great guys at Matterport tell us to do!</p>
<p>By the way, this script also saves a full-size copy of the image in the demo_captures folder, named "d1_capture.jpg". You will need this image for demo4.py to create zones using pixel coordinates.</p>
<figure style="text-align: center;">
<img style="height: 480px" src="README_images/readme09.png" alt="What Mask R-CNN sees">
<figcaption style="margin-top: 0; font-style: italic; font-size: smaller;">What Mask R-CNN sees</figcaption>
</figure>
<div style="background-color: lightgoldenrodyellow; padding: 6px;">
<p>
<b>NOTE</b> - After some installations, you may receive the following error when attempting to use cv2.imshow():</p>
<pre>
The function is not implemented. Rebuild the library with Windows, GTK+ 2.x or Carbon support.
</pre>
<p>To correct this problem, force reinstall OpenCV using the following commands:</p>
<pre>
pip3 install --upgrade --force-reinstall opencv-python --user
pip3 install --upgrade --force-reinstall opencv-contrib-python --user
</pre>
</div>
<hr>
<h3 id="d2">Demo 2 - Mask R-CNN Object Detection</h3>
<p>Speaking of Mask R-CNN, it is time to add it to the demo. As we stated earlier, we are using the Common Objects in Context (COCO) dataset for this demo. The COCO dataset has 80 classes of objects (lines 46-63), of which we want three: cars (#3), trucks (#6), and buses (#8):</p>
<pre>
# COCO Class names
# Index of the class in the list is its ID. For example, to get ID of
# the teddy bear class, use: class_names.index('teddy bear')
class_names = ['BG', 'person', 'bicycle', 'car', 'motorcycle', 'airplane',
'bus', 'train', 'truck', 'boat', 'traffic light',
'fire hydrant', 'stop sign', 'parking meter', 'bench', 'bird',
'cat', 'dog', 'horse', 'sheep', 'cow', 'elephant', 'bear',
'zebra', 'giraffe', 'backpack', 'umbrella', 'handbag', 'tie',
'suitcase', 'frisbee', 'skis', 'snowboard', 'sports ball',
'kite', 'baseball bat', 'baseball glove', 'skateboard',
'surfboard', 'tennis racket', 'bottle', 'wine glass', 'cup',
'fork', 'knife', 'spoon', 'bowl', 'banana', 'apple',
'sandwich', 'orange', 'broccoli', 'carrot', 'hot dog', 'pizza',
'donut', 'cake', 'chair', 'couch', 'potted plant', 'bed',
'dining table', 'toilet', 'tv', 'laptop', 'mouse', 'remote',
'keyboard', 'cell phone', 'microwave', 'oven', 'toaster',
'sink', 'refrigerator', 'book', 'clock', 'vase', 'scissors',
'teddy bear', 'hair drier', 'toothbrush']
</pre>
<p>However, Mask R-CNN runs through all of the classes, making it inefficient. Eventually, we hope to replace it with a smaller dataset focused on vehicles, which will make the processing faster. </p>
<p>The script loads a pre-trained model (line 44), collects the rgb_image from the camera, and then processes the image against the model, creating an array of detected objects (line 87):</p>
<pre>
# Load pre-trained model
model.load_weights(COCO_MODEL_PATH, by_name=True)
...
...
# Run the image through the Mask R-CNN model to get results.
results = model.detect([rgb_image], verbose=0)
</pre>
<p>We use the built-in visualize function to display the masks of these objects on the screen:</p>
<pre>
# Show the frame of video on the screen
mrcnn.visualize.display_instances(rgb_image, r['rois'], r['masks'], r['class_ids'], class_names, r['scores'])
</pre>
<figure style="text-align: center;">
<img style="height: 480px" src="README_images/readme10.png" alt="Mask R-CNN masking and scoring">
<figcaption style="margin-top: 0; font-style: italic; font-size: smaller;">Mask R-CNN masking and scoring</figcaption>
</figure>
<p>Once again, look at the code and note the changes between demo1.py and demo2.py, especially the imports. Also note that if the dataset you downloaded earlier is missing, the script will automatically download it from Matterport's GitHub repository, using mrcnn.utils on line 37. If you are behind a firewall, as we were at NASA, you can disabled this functionality with the following code:</p>
<pre>
# Do not download COCO trained weights from Releases if needed!
if not os.path.exists(COCO_MODEL_PATH):
# mrcnn.utils.download_trained_weights(COCO_MODEL_PATH)
print("Oh no! We are missing the mask_rcnn_coco.h5 dataset!")
</pre>
<hr>
<h3 id="d3">Demo 3 - Recognizing and Counting Vehicles</h3>
<p>In the previous demo, Mask R-CNN placed masks on every item it detected. However, as we stated earlier, we are only looking for vehicles. Therefore, we added the get_car_boxes function (line 25), which returns a numpy array of the cars (#3), trucks (#6), and buses (#8) that Mask R-CNN detected: </p>
<pre>
# Filter a list of Mask R-CNN detection results to get only the detected cars / trucks
def get_car_boxes(boxes, class_ids):
car_boxes = []
for i, box in enumerate(boxes):
# If the detected object isn't a car / truck, skip it
if class_ids[i] in [3, 8, 6]:
car_boxes.append(box)
return np.array(car_boxes)
</pre>
<p>We use that array with the OpenCV rectangle function to box the vehicles in (line 102):</p>
<pre>
# Draw the box
cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 255, 0), 2)
</pre>
<figure style="text-align: center;">
<img style="height: 480px" src="README_images/readme11.png" alt="Identifying vehicles">
<figcaption style="margin-top: 0; font-style: italic; font-size: smaller;">Identifying vehicles</figcaption>
</figure>
<p>We also acquired their locations in the frame, which we will use this later to determine if a vehicle is within a zone.</p>
<figure style="text-align: center;">
<img style="height: 480px" src="README_images/readme11b.png" alt="Raw counts and coordinates">
<figcaption style="margin-top: 0; font-style: italic; font-size: smaller;">Raw counts and coordinates</figcaption>
</figure>
<hr>
<h3 id="d4">Demo 4 - Creating and Displaying Zones</h3>
<p>Next, we create the zones using OpenCV. We will only count cars within these zones, to avoid counting cars on roads, other parking lots, etc. In addition, each of these zones will have a zone type (e.g., employee, handicap, etc.), which will allow us to provide better counts to the users.</p>
<p>Using the frame you captured in demo1.py and your favorite image viewer/editor, determine the pixel coordinates of each zone and enter them in the poly_coords array. For demo4.py, we created two zones, employee and handicap, and overlaid them on the frame:</p>
<pre>
# Read clockwise from top-left corner
poly_coords = ([[816, 1150], [3200, 1140], [3200, 1350], [816, 1400]],
[[240, 1140], [815, 1150], [815, 1400], [150, 1400]])

# BGR colors: Orange, Blue, Red, Gray, Yellow, Cyan, Pink, White
colors = [[0, 127, 255], [255, 0, 0], [0, 0, 255], [127, 127, 127],
[0, 255, 255], [255, 255, 0], [127, 0, 255], [255, 255, 255]]

# Make an overlay for transparent boxes
overlay = frame.copy()

# Draw the filled zones
for index, p in enumerate(poly_coords, start=0):
cv2.fillPoly(overlay, np.int32(
[np.array(p)]), colors[index + 4])

# Set transparency for boxes
alpha = 0.4
# Add overlay to frame
frame = cv2.addWeighted(overlay, alpha, frame, 1 - alpha, 0)

# Optional Draw the zone boundaries
for index, p in enumerate(poly_coords, start=0):
cv2.polylines(frame, np.int32(
[np.array(p)]), True, colors[index], 10)

# Draw center crosshair
height, width, channels = frame.shape
cv2.drawMarker(frame, (int(width / 2), int(height / 2)),
[255, 255, 0], cv2.MARKER_TRIANGLE_UP, 16, 2, cv2.LINE_4)
# Add timestamp
cv2.putText(frame, timestamp, (10, 30),
cv2.FONT_HERSHEY_SIMPLEX, 1, [0, 0, 255], 1)
</pre>
<figure style="text-align: center;">
<img style="height: 480px" src="README_images/readme12.png" alt="Creating and displaying zones">
<figcaption style="margin-top: 0; font-style: italic; font-size: smaller;">Creating and displaying zones</figcaption>
</figure>
<p>The cyan triangle in the middle is a calibration point. For Pan-Tilt-Zoom (PTZ) cameras, if the triangle and an outside reference point (e.g., a cone, sign, etc.) line up, the zone overlays are placed correctly on the frame. To see your work, this script also saves a time-stamped scaled copy of the image in the demo_captures folder, named "[YYMMDD]_d4_capture.jpg".</p>
<hr>
<h3 id="d5">Demo 5 - Counting Vehicles in Zones</h3>
<p>Now that we have our zones, we will run Mask R-CNN again, but we will use Shapely's intersects function to to determine if a vehicle is in a zone (line 127). If a vehicle is in a zone, it is added to the zone's count (line 132). However, is is deleted from the numpy array, to avoid double counting cars that may intersect more than one zone (line 134):</p>
<pre>
# Only show cars in the zones!
if((Polygon([(x1, y1), (x2, y1), (x1, y2), (x2, y2)])).intersects(Polygon(asPoint(array(p))))):
# Draw the box and add to overlay
cv2.rectangle(frame, (x1, y1), (x2, y2),
colors[index], 5)
# Count car in zone
count += 1
# Delete the car to avoid double counting
np.delete(car_boxes, box)
</pre>
<figure style="text-align: center;">
<img style="height: 480px" src="README_images/readme13.png" alt="Identifying vehicles within zones">
<figcaption style="margin-top: 0; font-style: italic; font-size: smaller;">Identifying vehicles within zones</figcaption>
</figure>
<figure style="text-align: center;">
<img style="height: 480px" src="README_images/readme14.png" alt="Counting vehicles within zones">
<figcaption style="margin-top: 0; font-style: italic; font-size: smaller;">Counting vehicles within zones</figcaption>
</figure>
<hr>
<h3 id="d6">Demo 6 - Creating the Database</h3>
<p>Like we stated earlier, due to architecture, security requirements, cross-lot tracking, etc., the NASA application was complex, and required two separate databases. For our demo, we will use a simpler data model, and, for portabililty (and to keep this README from being longer than it already is!) we'll use SQLite to hold the data.</p>
<p>As stated earlier, Park counts the number of vehicles in parking zones. We found that a single camera cannot always observe a complete lot. Instead, a camera can observe portions of multiple lots and a lot can be observed by multiple cameras. Therefore, we divided each feed into zones, with each zone associated with a lot, and, as shown in demo4.py, each zone is classified as employee parking, handicap parking, etc. In demo7.py. we will show you how Park cycles through and analyzes all the feeds every five minutes, collecting the counts for the zones that each camera can observe. Once the cycle is complete, Park can aggregate the counts into lot totals, which the front end can present to the user.</p>
<p>Here's the data model:</p>
<figure style="text-align: center;">
<img style="height: 480px" src="README_images/readme15.png" alt="Park Data Model">
<figcaption style="margin-top: 0; font-style: italic; font-size: smaller;">Park Data Model </figcaption>
</figure>
<p>And here are the table descriptions:</p>
<ul>
<li><b>Source:</b> This table contains a list of the sources, such as cameras, that collect images, as well as their location and credentials. The camera's URL is a unique value.</li>
<li><b>Lot:</b> This table contains a list of parking lots, as well as their location in latitude and longitude. The lot name is a unique value.</li>
<li><b>Type:</b> This table contains a list of zone types, such as employee parking, handicap parking, etc. The description is a unique value.</li>
<li><b>Zone:</b> This table contains all the zones from all the lots in the database. It also contains the total number of parking spaces within each zone, as well as the boundaries of the zone within its source's frame. This table has a one-to-many relationship with Source, Lot, and Type, and a unique compound key comprised of the primary keys of each of those tables and its own ZoneID.</li>
<li><b>OccupancyLog:</b> This table is a junction table (i.e., an associative entity) that collects and timestamps all the zone counts, providing both current and historical parking data. This table has a one-to-many relationship with Lot, Zone, and Type, and a unique compound key comprised of the primary keys of each of those tables and its own Timestamp.</li>
</ul>
<p>Run demo6.py to create the database in the db folder and open the database using the following commands:</p>
<pre>
[park@localhost Park]$ ./demo6.py
[park@localhost Park]$ ls db
[park@localhost Park]$ sqlite3 db/park.db
</pre>
<figure style="text-align: center;">
<img style="height: 480px" src="README_images/readme16.png" alt="Creating the Database">
<figcaption style="margin-top: 0; font-style: italic; font-size: smaller;">Creating the Database </figcaption>
</figure>
<p>Once complete, test the new database with the some queries. Here are some examples; if they do not work, check for any errors that may have occurred when you created the database:</p>
<pre>
SELECT * FROM Lot ORDER BY Name ASC;
SELECT * FROM Lot WHERE LotID = 1;
SELECT * FROM Type ORDER BY TypeID ASC;
SELECT * FROM Source ORDER BY SourceID ASC;
SELECT Zone.ZoneID, Source.Location, Type.Description, Lot.Name, Zone.TotalSpaces, Zone.PolyCoords
FROM Zone
INNER JOIN Source ON Zone.SourceID = Source.SourceID
INNER JOIN Type ON Zone.TypeID = Type.TypeID
INNER JOIN Lot ON Zone.LotID = Lot.LotID
ORDER BY Lot.Name, Type.Description;
</pre>
<figure style="text-align: center;">
<img style="height: 480px" src="README_images/readme17.png" alt="Running test queries">
<figcaption style="margin-top: 0; font-style: italic; font-size: smaller;">Running test queries </figcaption>
</figure>
<hr>
<h3 id="d7">Demo 7 - Putting It All Together.</h3>
<figure style="text-align: center;">
<img src="README_images/park_demo.gif" alt="Putting it all together">
<figcaption style="margin-top: 0; font-style: italic; font-size: smaller;">Putting it all together</figcaption>
</figure>
<p>Finally! demo7.py is a super demo. It incorporates all of the functionality of the previous demos, and actually connects to the database to get feed credentials and to update data.</p>
<div style="background-color: lightgoldenrodyellow; padding: 6px;">
<p>
<strong>By the way, demo7.py requires user interaction after displaying each frame. The above image is an animation of the demo7.py results, created using GIMP.</strong>
</p>
</div>



<pre>
    # Update front_end_lot table
    sql = "TRUNCATE front_end_lot;";
    cursor.execute(sql)
    sql = ("INSERT INTO " \
    "front_end_lot (lotid, lotname, employee, employee_max, handicap, handicap_max, visitor, visitor_max, efficient, efficient_max, shortterm, " \
    "shortterm_max, latitude, longitude, MinX, MinY, MaxX, MaxY, timestamp) " \
    "SELECT r_occupancy_log.lot_id AS lotid, r_lot.lot_name AS lotname, " \
    "sum(case when r_occupancy_log.zone_type_id = '1' AND r_occupancy_log.timestamp = '{0}' then r_occupancy_log.snapshot_occ else 0 end) AS employee, " \
    "sum(case when r_occupancy_log.zone_type_id = '1' AND r_occupancy_log.timestamp = '{0}' then r_occupancy_log.snapshot_max else 0 end) AS employee_max, " \
    "sum(case when r_occupancy_log.zone_type_id = '2' AND r_occupancy_log.timestamp = '{0}' then r_occupancy_log.snapshot_occ else 0 end) AS handicap, " \
    "sum(case when r_occupancy_log.zone_type_id = '2' AND r_occupancy_log.timestamp = '{0}' then r_occupancy_log.snapshot_max else 0 end) AS handicap_max, " \
    "sum(case when r_occupancy_log.zone_type_id = '3' AND r_occupancy_log.timestamp = '{0}' then r_occupancy_log.snapshot_occ else 0 end) AS visitor, " \
    "sum(case when r_occupancy_log.zone_type_id = '3' AND r_occupancy_log.timestamp = '{0}' then r_occupancy_log.snapshot_max else 0 end) AS visitor_max, " \
    "sum(case when r_occupancy_log.zone_type_id = '4' AND r_occupancy_log.timestamp = '{0}' then r_occupancy_log.snapshot_occ else 0 end) AS efficient, " \
    "sum(case when r_occupancy_log.zone_type_id = '4' AND r_occupancy_log.timestamp = '{0}' then r_occupancy_log.snapshot_max else 0 end) AS efficient_max, " \
    "sum(case when r_occupancy_log.zone_type_id = '5' AND r_occupancy_log.timestamp = '{0}' then r_occupancy_log.snapshot_occ else 0 end) AS shortterm, " \
    "sum(case when r_occupancy_log.zone_type_id = '5' AND r_occupancy_log.timestamp = '{0}' then r_occupancy_log.snapshot_max else 0 end) AS shortterm_max, " \
    "r_lot.lat AS latitude, r_lot.lon AS longitude, r_lot.MinX AS MinX, r_lot.MinY AS MinY, r_lot.MaxX AS MaxX, r_lot.MaxY AS MaxY, '{0}' AS timestamp " \
    "FROM r_occupancy_log " \
    "INNER JOIN r_lot ON r_occupancy_log.lot_id = r_lot.lot_id " \
    "WHERE r_occupancy_log.timestamp = '{0}'" \
    "GROUP BY r_occupancy_log.lot_id " \
    "ORDER BY r_lot.lot_name ASC;".format(timestamp))
    </pre>
        <p>You must have one active camera for it to work (actually, you will need an active camera for LaRC
            Park to work at all!), and the enclosed database script should set you up for success. Once you
            run
            the script, look for the latest entries in the r_occupancy_log and front_end_lot data tables.
            They
            should match the counts the demo came up with. Remember, r_occupancy_log, which is not
            accessible
            outside the NASA firewall, holds current and historical data, while front_end_lot, which is
            accessible to users, only holds the latest counts.</p>
        <figure style="text-align: center;">
            <img src="LaRC%20Park%20Tutorial_files/image025.jpg" alt="Occupancy Log data table updated">
            <figcaption style="margin-top: 0; font-style: italic; font-size: smaller;">Figure 25. Occupancy
                Log
                data table updated</figcaption>
        </figure>
        <figure style="text-align: center;">
            <img src="LaRC%20Park%20Tutorial_files/image026.jpg" alt="Front-End data table updated">
            <figcaption style="margin-top: 0; font-style: italic; font-size: smaller;">Figure 26. Front-End
                data
                table updated</figcaption>
        </figure>
        <p>Before you continue, we recommend you look over both the park-d_engine.py and frame-d_capture.py
            scripts. You should have a good understanding on how they work. Our next step will be to have
            them
            automatically update the database and image folders.</p>
        <br>
        <hr>
        <br>
        <h2 id="p5">Step 4: Add and Schedule the Scripts in cron</h2>
        <p>If LaRC Park had to run Mask R-CNN and Tensorflow each time it was opened, as well as get video
            feeds, the site would be <strong><em>SLOWWWWW!</em></strong> Instead, we use cron, the Linux
            task
            scheduler, to do all that work in the background, and use the database as the intermediary
            between
            the cron daemon and the front end. As stated before, cron will runengine.py every five
            minutes, between 0400 and 1859, Monday thru Friday, and run frame_capture.py every day at 1000,
            Monday thru Friday.</p>
        <p>Log back into the cron folder on the server using PuTTY with the following command:</p>
        <pre>sudo su - cron-park-d</pre>
        <p>Copy the files and directories in the <b>scripts</b> folder of this repository into the
            cron-park-d
            folder. Due to cron folder permissions, you may have to copy the files to your local folder or
            htdocs/park-d first, and then copy them into cron-park-d, using commands similar to the
            following:
        </p>
        <pre>
    cp -r /home/[Your AUID]/scripts/* /home/cron-park-d
    - or -
    cp -r /usr/local/web/htdocs/park-d/scripts/* /home/cron-park-d
    </pre>
        <p>Next, get rid of the demo scripts and folders using the following commands; as we stated earlier,
            you
            cannot execute them in cron:</p>
        <pre>
    rm -r test_images
    rm -r test_videos
    rm -r park-d_demo*
    </pre>
        <p>Look at the directory contents and at the directory structure using the following commands:</p>
        <pre>
        ls
        tree # if tree is not installed use ls -R | grep ":$" | sed -e 's/:$//' -e 's/[^-][^\/]*\//--/g' -e 's/^/   /' -e 's/-/|/'
    </pre>
        <figure style="text-align: center;">
            <img src="LaRC%20Park%20Tutorial_files/image027.jpg" alt="cron directory contents and tree">
            <figcaption style="margin-top: 0; font-style: italic; font-size: smaller;">Figure 27. cron
                directory contents and tree</figcaption>
        </figure>
        <p>The files are:</p>
        <ul>
            <li>config.ini - Contains encryption keys and the credentials needed to connect to the
                database.
            </li>
            <li>frame_capture.py - Captures and saves an image of the video feed, overlaid with zone
                boundaries. This script is used for camera calibration and maintenance.</li>
            <li>Mask_RCNN - The directory containing the Mask R-CNN scripts.</li>
            <li>mask_rcnn_coco.h5 - The file containing the pre-trained COCO weights for TensorFlow and
                Keras.</li>
            <li>park_engine.py - Uses machine learning and image recognition to detect and count cars
                trucks
                and buses in specific zones within a video feed. This script is used to update the
                counts in
                the database for each zone.</li>
            <li>permissions.sh - A script to change the permission of the other scripts to executable.
            </li>
        </ul>
        <p>First, ensure <span>permissions.sh</span> is executable, and then reset permissions on all
            the
            other scripts using the following commands:</p>
        <pre>
    chmod 755 permissions.sh
    ./permissions.sh
    </pre>
        <p>Now we will edit the cron table. On our system, entering the following command opens the
            table
            using the Vim text editor (if you want to use nano, ed or emacs, talk to your system
            administrator, because select-editor does not work):</p>
        <pre>crontab -e</pre>
        <p>Go into insert mode by pressing [i], then add the following lines:</p>
        <pre>
    */5 04-18 * * 1-5 /home/cron-park-d/park-d_engine.py
    0   10    * * 1-5 /home/cron-park-d/frame-d_capture.py
    </pre>
        <figure style="text-align: center;">
            <img src="LaRC%20Park%20Tutorial_files/image028.jpg" alt="crontab contents">
            <figcaption style="margin-top: 0; font-style: italic; font-size: smaller;">Figure 28.
                crontab
                contents</figcaption>
        </figure>
        <p>We built a little cheat sheet into ours; you can do the same. Press [ESC] to return to
            command
            mode, then enter ":wq" to write out and quit. Wait about 5 to 15 minutes and enter the
            following
            commands</p>
        <pre>
    cron-tab -l
    mail
    </pre>
        <figure style="text-align: center;">
            <img src="LaRC%20Park%20Tutorial_files/image029.jpg" alt="Active cron jobs">
            <figcaption style="margin-top: 0; font-style: italic; font-size: smaller;">Figure 29. Active
                cron jobs and mail</figcaption>
        </figure>
        <p>Wow! crontab -l shows a listing of all active cron jobs, and mail brings up the results of
            the
            cron jobs. As you can see, we have been running the scripts for a while. Type in the number
            of
            messages and the newest mail should open up:</p>
        <figure style="text-align: center;">
            <img src="LaRC%20Park%20Tutorial_files/image030.jpg" alt="cron job mail (top)">
            <figcaption style="margin-top: 0; font-style: italic; font-size: smaller;">Figure 30. cron
                job
                mail (top)</figcaption>
        </figure>
        <figure style="text-align: center;">
            <img src="LaRC%20Park%20Tutorial_files/image031.jpg" alt="cron job mail (bottom)">
            <figcaption style="margin-top: 0; font-style: italic; font-size: smaller;">Figure 31. cron
                job
                mail (bottom)</figcaption>
        </figure>
        <p>As you can see, cron job mail contains the results of running the scripts. In this case, the
            script successfully counted vehicles in zone, but you should monitor the database and the
            cron
            job mail for any errors due to system changes or patches, etc. By the way, inputing exit
            once
            will exit mail, entering exit again will exit cron, and entering exit one more time will
            exit
            PuTTY.</p>
        <p>Congratulations! The system is now on autopilot. Next, we will go over how to add, update,
            and
            delete everything from buildings to zones.</p>
        <br>


<hr>
<div style="width: 100%; text-align: center;">
<img src="sherpa_logo.png" alt="Sherpa Logo" style="height:240px;" />
</div>
<p style="text-align: center;">The Sherpas are Jrei Dimanno, Justine Forrest, Rob Garcia, John Graham, Tanner Griffin, Ryan Hashi, Brandon Hutton, Gabriel Jacobs, Fernando Jauregui, Wesley Madden, and Doug Trent</p>
<hr>
<h2 id="p10">References</h2>
<p>Geitgey, A. (2019, January 21). Snagging parking spaces with Mask R-CNN and Python. Retrieved from <a href="https://medium.com/@ageitgey/snagging-parking-spaces-with-mask-r-cnnand-python-955f2231c400" target="_blank" title="Snagging parking spaces with Mask R-CNN and Python">https://medium.com/@ageitgey/snagging-parking-spaces-with-mask-r-cnnand-python-955f2231c400 </p>
</a>
<p>INRIX. (2017, July 12). Searching for parking costs Americans $73 billion a year. Retrieved from <a href="http://inrix.com/press-releases/parking-pain-us/" target="_blank" title="Searching for parking costs Americans $73 billion a year">http://inrix.com/press-releases/parking-pain-us/ </p>
</a>
<p>International Parking and Mobility Institute. (2015, October 12). Open requests for proposals. Retrieved July 31, 2019, from <a href="https://www.parking.org/membership/member-resources/rfps/" target="_blank" title="Open requests for proposals">https://www.parking.org/membership/member-resources/rfps/ </p>
</a>
<p>Matterport, Inc. (2019, March 10). matterport/Mask_RCNN. Retrieved August 2, 2019, from <a href="https://github.com/matterport/Mask_RCNN" target="_blank" title="matterport/Mask_RCNN">https://github.com/matterport/Mask_RCNN</a>
</p> 
</body>

</html>