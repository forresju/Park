# Park!
<div style="text-align: center;">
<img src="README_images/park_demo.gif" alt="Welcome to Park!" style="width:100%;" />
</div>
<h2>Table of Contents</h2>
<ul>
<li><a href="#p1" title="Introduction">Introduction</a></li>
<li><a href="#p2" title="Setting Up the Development Environment">Setting Up the Development Environment</a></li>
<li><a href="#p3" title="Adding the Machine Learning Packages">Adding the Machine Learning Packages</a></li>
<li><a href="#p4" title="Running the Scripts">Running the Scripts</a></li>
<ul>
<li><a href="#d1" title="OpenCV Image Capture and Display">OpenCV Image Capture and Display</a></li>
<li><a href="#d2" title="Mask R-CNN Object Detection">Mask R-CNN Object Detection</a></li>
<li><a href="#d3" title="Recognizing and Counting Vehicles">Recognizing and Counting Vehicles</a></li>
<li><a href="#d4" title="Creating and Displaying zones">Creating and Displaying Zones</a></li>
<li><a href="#d5" title="Counting Vehicles in Zones">Counting Vehicles in Zones</a></li>
<li><a href="#d6" title="Creating the Database">Creating the Database</a></li>
<li><a href="#d7" title="El Super Demo">El Super Demo</a></li>
</ul>
<li><a href="#p5" title="Schedule the Scripts Using cron">Schedule the Scripts Using cron</a></li>
<li><a href="#p10" title="References">References</a>
</ul>
<hr>
<h2 id="p1">Introduction</h2>
<p>As interns at NASA Langley, aka "The Sherpas", we worked on a lot of projects. Some of them involved machine learning, and, yes, this included a parking lot project.</p>
<p>The use case was that finding parking is a problem for everyone. In 2016, drivers in New York City spent an average of 107 hours and $2243 a year looking for parking (INRIX, 2017). In addition, there are over 120 outstanding requests for proposals for city and institutional parking management (IPMI, 2019). One of the original Sherpas, who had been working on a similar project at Georgia Tech, suggested creating a parking management system for NASA Langley's Digital Transformation initiative, and it was approved.</p>
<p>We looked at various solutions, including sensors in each parking space; Light Detection and Ranging (LIDAR) to count cars entering and exiting parking areas; etc. Each approach had issues that made it impractical to implement with the resources we had; for example, most image recognition systems we looked at required a high, almost overhead, angle of view.</p>
<p>Finally, we came across <a href="https://medium.com/@ageitgey/snagging-parking-spaces-with-mask-r-cnnand-python-955f2231c400" target="_blank" title="Snagging parking spaces with Mask R-CNN and Python">Adam Geitgey's awesome article on object detection</a>, which introduced us to <a href="https://github.com/matterport/Mask_RCNN" target="_blank" title="Matterport Mask R-CNN">Matterport Mask R-CNN</a> (Geitgey, 2019). We realized that his implementation of Mask R-CNN was something we could use with the resources available to us, especially the lower and more practical viewing angle. However, Adam’s solution, designed for a single system for a single user looking at a single parking lot, was too expensive in terms of processing power and time. The first time we implemented it, it sent our cooling fans into overdrive and hung our computers for several minutes; run another of our projects, <a href="http://acronymsfortina.rgprogramming.com/index.html" target="_blank" title="Acronyms for Tina">Acronyms for Tina</a>, on localhost for a similar, albeit less intense, experience.</p>
<p>Still, it worked, so using what we learned, we went back to the drawing board to develop a system that would rotate through multiple lots; analyze and collect parking data, and present this information to users in a timely manner. After much trial and error, we came up with a system that leveraged Linux scripting, Python, OpenCV, TensorFlow, SQL, and PHP to:</p>
<ol>
<li>Use a cron task scheduler to access the cameras overlooking each lot in turn.</li>
<li>Capture a frame from each camera; divide it into zones (e.g., employee, handicap, etc.); and count the number of vehicles in each zone.</li>
<li>Aggregate the results and collect them in a database.</li>
<li>Have the front-end pull data for users from the database.</li>
</ol>
<p>Due to architecture, security requirements, cross-lot tracking, etc., the NASA application was a bit complex, and for those same reasons, we will not recreate it here. However, here are a series of demos, written in Python, that breakdown the way the machine learning aspect works. Besides a practical application of TensorFlow, Mask R-CNN, and computer vision, this is also a very good introduction to the Common Objects in COntext (COCO) dataset. Have fun and good luck!</p>
<hr>
<h2 id="p2">Setting Up the Development Environment</h2>
<p>As we just said, we'll be using CentOS Linux 7 in VirtualBox for this demo, but you can use another virtual machine or an actual server if you like. Just make sure that your system has at least 2GB of memory; 16GB of hard disk space; 128MB of video memory; and a connection to the Internet.</p>
<div style="background-color: lightgoldenrodyellow; padding: 6px;">
<p>
<b>NOTE</b> - If you do choose to use CentOS or Red Hat Linux, we've included another README file name CENTOS with directions on how to set up your environment.</p>
</div>
<div style="background-color: lightgoldenrodyellow; padding: 6px;">
<p>
<b>NOTE</b> - While the production code for NASA only generated data, the demo scripts also display images. To view these images, you will need an X Server. For these demos, our solution was to use the GNOME Graphical User Interface (GUI) for development; it comes with a display server; it's lightweight; and it allows you to cut-and-paste from the host machine into the VM's Terminal. To use GNOME on CentOS or Red Hat Linux, use the following commands:</p>
<pre>
sudo yum -y groupinstall "GNOME Desktop"
sudo startx
</pre>
</div>
<p>Our first step is to make sure the operating system is up to date. On CentOS, we would use the following command; other flavors of Linux may use "sudo apt-get update" instead, while Windows users can run "wuauclt.exe /updatenow":</p>
<pre>
[park@localhost ~]# sudo yum -y update
</pre>
<p>This may take a while, especially on a new system.</p>
<p>Once the system update is completed, make sure that the tools needed for development are installed:</p>
<ol>
<li><b>Python 3 Programming Language Interpreter and PIP Python Package Installer</b> - While Python 2 is installed with CentOS by default, we will need Python 3 to run our computer vision and machine learning scripts, specifically Python 3.6.x. There are a few ways of doing this, but we will use the IUS Community Repo; for an in-depth look at options, check out <a href="https://www.hogarthuk.com/?q=node/15" title="Running newer applications on CentOS" target="_blank">this link from James Hogarth</a>. To install Python, run the following command: 
<pre>
[park@localhost ~]# sudo yum -y install https://centos7.iuscommunity.org/ius-release.rpm
[park@localhost ~]# sudo yum -y install python36u
[park@localhost ~]# sudo yum -y install python36u-pip
[park@localhost ~]# sudo yum -y install python36u-devel
[park@localhost ~]# python3 --version
[park@localhost ~]# pip3 --version
</pre>
</li>
<li><b>SQLite RDBMS</b> - For portability (and to keep this README as short as possible), we'll be using SQLite. Check the version using the following command: 
<pre>
[park@localhost ~]# sqlite3 -version
3.7.17 2013-05-20 00:56:22 118a3b35693b134d56ebd780123b7fd6f1497668
</pre>
<p>If SQLite is not installed, install it using the following command:</p>
<pre>
[park@localhost ~]# sudo yum -y install sqlite
</pre>
</li>
<li><b>cron Time-Based Job Scheduler</b> - cron should already be installed by default, but check anyway:
<pre>
[park@localhost ~]# whereis -b crontab | cut -d' ' -f2 | xargs rpm -qf
cronie-1.4.11-19.e17.x86_64
</pre>
<p>If cron is not installed, install it using the following command:</p>
<pre>
[park@localhost ~]# yum -y install cronie
</pre>
</li>
</ol>
<p>Alright! Before continuing, let's do another update of the system using the following command:</p>
<pre>
[park@localhost ~]# sudo yum -y update
</pre>
<p>Just in case, we'll double check everything is installed and updated using the following commands:</p>
<pre>
[park@localhost ~]# python3 ––version
[park@localhost ~]# pip3 ––version
[park@localhost ~]# sqlite3 -version
[park@localhost ~]# whereis -b crontab | cut -d' ' -f2 | xargs rpm -qf
</pre>
<div style="text-align: center;">
<img style="height: 480px" src="README_images/readme01.png" alt="Verifying initial setup" />
<p style="margin-top: 0; font-style: italic; font-size: smaller;">Verifying initial setup </p>
</div>
</div>
<hr>
<h2 id="p3">Adding the Machine Learning Packages</h2>
<p>Our next step is to add the packages we will need to run the scripts.</p>
<div style="background-color: lightgoldenrodyellow; padding: 6px;">
<p>
<b>NOTE</b> - If you like, you can clone this repository into your folder: <strong>Just make sure you do so before installing the ML packages.</strong> Use the following command to clone the repo:</p>
<pre>
git clone https://github.com/garciart/Park.git
</pre>
<p>This will create a folder "Park" with all the code in the right place. If you are using a shared folder, fetch into your shared folder instead of cloning:</p>
<pre>
[park@localhost ~]# cd Park # replace Park with the name of your shared folder
[park@localhost Park]$ git init
[park@localhost Park]$ git remote add origin https://github.com/garciart/Park.git
[park@localhost Park]$ git fetch
[park@localhost Park]$ git checkout origin/master -ft
</pre>
<p>As a rule, we don't use the "Master" branch, just like we don't use "root" for development. You don't have to, but we suggest creating a separate branch:</p>
<pre>
[park@localhost ~]# cd Park # replace Park with the name of your shared folder
[park@localhost Park]$ git branch park # replace park with your username
[park@localhost Park]$ git checkout park # replace park with your username
[park@localhost Park]$ git status
</pre>
<p>In addition, remember to use "Park" as your development folder, not the home folder (in our case, "park"). </p>
</div>
<div style="text-align: center;">
<img style="height: 480px" src="README_images/readme02.png" alt="Cloning the Park repository" />
<p style="margin-top: 0; font-style: italic; font-size: smaller;">Cloning the Park repository </p>
</div>
</div>
<p>Clone the excellent <a href="https://github.com/matterport/Mask_RCNN" target="_blank" title="Mask R-CNN engine from Matterport">Mask R-CNN engine from Matterport</a>, which will serve as our object detection and instance segmentation engine using the following commands:</p>
<pre>
[park@localhost Park]$ git clone https://github.com/matterport/Mask_RCNN.git
[park@localhost Park]$ cd Mask_RCNN
[park@localhost Mask_RCNN]$ ls
[park@localhost Mask_RCNN]$ cat requirements.txt
</pre>
<div style="text-align: center;">
<img style="height: 480px" src="README_images/readme03.png" alt="Cloning the Mask R-CNN repository" />
<p style="margin-top: 0; font-style: italic; font-size: smaller;">Cloning the Mask R-CNN repository </p>
</div>
</div>
<p>Note the setup.py file, which will build and install the Mask RCNN engine, and the requirements.txt file, which contains the list of modules and packages we will need to run the Mask RCNN engine:</p>
<ul>
<li><a href="https://numpy.org/" title="numpy" target="_blank">numpy</a> - support for high-level mathematics involving arrays and matrices.</li>
<li><a href="https://scipy.org/scipylib" title="scipy" target="_blank">scipy</a> - support for scientific and technical computing.</li>
<li><a href="https://python-pillow.org/" title="Pillow" target="_blank">Pillow</a> - Python Imaging Library to open, manipulate and save image files.</li>
<li><a href="https://cython.org/" title="cython" target="_blank">cython</a> - support for C extensions and datatypes in Python.</li>
<li><a href="https://matplotlib.org/" title="matplotlib" target="_blank">matplotlib</a> - support for 2d plotting.</li>
<li><a href="https://scikit-image.org/" title="scikit-image" target="_blank">scikit-image</a> - support for image processing.</li>
<li><a href="https://www.tensorflow.org/" title="tensorflow" target="_blank">tensorflow>=1.3.0</a> - support for machine learning and tensor mathematics.</li>
<li><a href="https://keras.io/" title="keras" target="_blank">keras>=2.0.8</a> - support for neural networks.</li>
<li><a href="https://opencv.org/" title="opencv" target="_blank">opencv-python</a> - support for real-time computer vision.</li>
<li><a href="https://www.h5py.org/" title="h5py" target="_blank">h5py</a> - support for implementing HDF5 binary data format.</li>
<li><a href="https://imgaug.readthedocs.io/en/latest/" title="imgaug" target="_blank">imgaug</a> - support for for image augmentation in machine learning.</li>
<li><a href="https://ipython.org/" title="IPython" target="_blank">IPython[all]</a> - support for interactive computing.</li>
</ul>
<p>However, before executing installing requirements.txt, we have to make some changes. Unfortunately, Matterport's version of Mask R-CNN cannot use TensorFlow 2.0 or Keras 2.3 or above. In addition, we will need to install <a href="https://docs.opencv.org/master/" title="opencv-python-contrib" target="_blank">OpenCV's extra modules</a> to display images. Using your favorite editor, change the TensorFlow and Keras lines to the following:</p>
<pre>
tensorflow&gt;=1.3.0,&lt;2.0
keras&gt;=2.0.8,&lt;2.3
</pre>
<p>In addition, add the following line after the OpenCV line:</p>
<pre>
opencv-contrib-python
</pre>
<div style="text-align: center;">
<img style="height: 480px" src="README_images/readme04.png" alt="Editing requirements.txt" />
<p style="margin-top: 0; font-style: italic; font-size: smaller;">Editing requirements.txt </p>
</div>
</div>
<p>We also have to make sure our <a href="https://setuptools.readthedocs.io/en/latest/" title="Setuptools" target="_blank">Setuptools</a> are up-to-date. Otherwise, we may run into errors extracting and creating modules and packages from requirements.txt. In addition, we have to make sure <a href="https://docs.python.org/3/library/tkinter.html" title="tkinter" target="_blank">tkinter, the standard Python interface to the Tk GUI toolkit</a>, is installed as well, or you may get a "matplotlib is currently using a non-GUI backend" error. Therefore, enter the following commands: </p>
<pre>
[park@localhost Mask_RCNN]$ pip3 install --upgrade setuptools --user
[park@localhost Mask_RCNN]$ sudo yum -y install python3-tkinter
</pre>
<p>Once the installation is complete, install Mask R-CNN's requirements using the following command:</p>
<pre>
[park@localhost Mask_RCNN]$ pip3 install -r requirements.txt --user
</pre>
<div style="text-align: center;">
<img style="height: 480px" src="README_images/readme05.png" alt="Installing Mask R-CNN requirements" />
<p style="margin-top: 0; font-style: italic; font-size: smaller;">Installing Mask R-CNN requirements</p>
</div>
</div>
<p>Hopefully, everything went well; if not, check the verbose comments for any errors and correct them. Remember, you may have to use a different flavor of a package depending on the system you are using. For example, we used Red Hat Linux, so for some packages, we had to use their version (e.g., python36-mysql, etc.). You can look for packages using:</p>
<pre>
[park@localhost Mask_RCNN]$ pip3 list
- or -
[park@localhost Mask_RCNN]$ yum search [package name]
</pre>
<p>Once everything is set, run the setup script using the command:</p>
<pre>
[park@localhost Mask_RCNN]$ python3 setup.py install --user
</pre>
<div style="text-align: center;">
<img style="height: 480px" src="README_images/readme06.png" alt="Setting up Mask R-CNN" />
<p style="margin-top: 0; font-style: italic; font-size: smaller;">Setting up Mask R-CNN </p>
</div>
</div>
<p>Once again, check the verbose comments for any errors and correct them. Finally, go back one directory and download the Common Objects in Context (COCO) dataset by using the following commands:</p>
<pre>
[park@localhost Mask_RCNN]$ cd ..
[park@localhost Park]$ wget https://github.com/matterport/Mask_RCNN/releases/download/v2.0/mask_rcnn_coco.h5
</pre>
<p>The COCO dataset consists of over 100,000 images of 80 objects, including cars (#3), buses (#6), and trucks (#8); find out more at <a href="http://cocodataset.org" title="COCO Dataset" target="_blank">http://cocodataset.org</a>. Please note that you can replace this dataset with a more specific set for vehicle detection, but for now, we will use the COCO dataset: </p>
<div style="text-align: center;">
<img style="height: 480px" src="README_images/readme07.png" alt="Getting the COCO dataset" />
<p style="margin-top: 0; font-style: italic; font-size: smaller;">Getting the COCO dataset </p>
</div>
</div>
<hr>
<h2 id="p4">Running the Scripts</h2>
<p>Now comes the fun part: Our Python code is a combination of Matterport's open-source Mask-RCNN's samples (Matterport, 2019), Alex Geitgey's excellent article on detecting open parking spaces (Geitgey, 2019), and our own embellishments (all MIT licensed, of course). By the time you are done working through these demos, you should have a good understanding how the heavy lifting occurs on the back end.</p>
<div style="background-color: lightgoldenrodyellow; padding: 6px;">
<p>
<b>NOTE</b> - To view these images generated by the demo scripts, you will need an X Server; otherwise, OpenCV will display a "Failed to Start the X server" error. Our solution was to use the GNOME Graphical User Interface (GUI) for development; it came with a display server; it was lightweight; and it allowed us to cut-and-paste from the host machine into the VM's Terminal. To do so on Linux, use the following commands:</p>
<pre>
sudo yum -y groupinstall "GNOME Desktop"
sudo startx
</pre>
</div>
<p>Download the demos to your development folder if you did not clone or fetch the repo. There are seven demos, and each one adds new functionality to Park.</p>
<ul>
<li>demo1.py - OpenCV image capture and display demo.</li>
<li>demo2.py - Adds Mask R-CNN object detection to the demo.</li>
<li>demo3.py - Adds recognizing and counting vehicles to the demo.</li>
<li>demo4.py - Adds creating and displaying zones to the demo.</li>
<li>demo5.py - Adds counting vehicles in zones to the demo.</li>
<li>demo6.py - Creates the database.</li>
<li>demo7.py - El Super Demo: Incorporates communicating with the database and the functionality of all of the previous demos.</li>
</ul>
<p>You will also need the log, capture, image, and video folders. If you cloned or fetched the repository, your directory should look like this:</p>
<div style="text-align: center;">
<img style="height: 480px" src="README_images/readme08.png" alt="Development folder listing" />
<p style="margin-top: 0; font-style: italic; font-size: smaller;">Development folder listing </p>
</div>
</div>
<p>Once you are all set up, open the code for each demo in your favorite editor as we get to it and follow along. By the way, each script has its own "shebang", so you can run them directly by setting their permissions to executable.</p>
<hr>
<h3 id="d1">Demo 1 - OpenCV Image Capture and Display</h3>
<p>This one is pretty easy, but using OpenCV is the foundation of Park's image recognition system. Note that we listed four FRAME_SOURCE's (Lines 16-20), with three of them commented out:</p>
<pre>
# Image, video or camera to process - set this to 0 to use your webcam instead of a file
# FRAME_SOURCE = [(IMAGE_DIR + "/demo_image1.jpg"),(IMAGE_DIR + "/demo_image2.jpg"),(IMAGE_DIR + "/demo_image3.jpg")]
# FRAME_SOURCE = [(VIDEO_DIR + "/demo_video1.mp4"),(VIDEO_DIR + "/demo_video2.mp4"),(VIDEO_DIR + "/demo_video3.mp4")]
# FRAME_SOURCE = [(IMAGE_DIR + "/demo_image.jpg")]
FRAME_SOURCE = ["https://raw.githubusercontent.com/garciart/Park/master/demo_images/demo_image.jpg"]
</pre>
<p>Park can work with images, videos and live streams. While we use demo_image.jpg, feel free to switch sources during the first three demos. However, to work correctly with this tutorial, Demos 4 through 5 need to use demo_image.jpg, while Demo 7 pulls what it needs from the database you will create in Demo 6. </p>
<p>Note the colors. OpenCV uses a Blue-Green-Red (BGR) color model, which we converted to an RGB color model for Mask R-CNN (Line 34):</p>
<pre>
# Convert the image from BGR color (which OpenCV uses) to RGB color
rgb_image = frame[:, :, ::-1]
</pre>
<p>We did not notice a difference in detection accuracy when we ran the other scripts, but we do what the great guys at Matterport tell us to do!</p>
<p>By the way, this script also saves a full-size copy of the image in the demo_captures folder, named "d1_capture.jpg". You will need this image for demo4.py to create zones using pixel coordinates.</p>
<div style="text-align: center;">
<img style="height: 480px" src="README_images/readme09.png" alt="What Mask R-CNN sees">
<p style="margin-top: 0; font-style: italic; font-size: smaller;">What Mask R-CNN sees</p>
</div>
<div style="background-color: lightgoldenrodyellow; padding: 6px;">
<p>
<b>NOTE</b> - After some installations, you may receive the following error when attempting to use cv2.imshow():</p>
<pre>
The function is not implemented. Rebuild the library with Windows, GTK+ 2.x or Carbon support.
</pre>
<p>To correct this problem, force reinstall OpenCV using the following commands:</p>
<pre>
pip3 install --upgrade --force-reinstall opencv-python --user
pip3 install --upgrade --force-reinstall opencv-contrib-python --user
</pre>
</div>
<hr>
<h3 id="d2">Demo 2 - Mask R-CNN Object Detection</h3>
<p>Speaking of Mask R-CNN, it is time to add it to the demo. As we stated earlier, we are using the Common Objects in Context (COCO) dataset for this demo. The COCO dataset has 80 classes of objects (lines 46-63), of which we want three: cars (#3), trucks (#6), and buses (#8):</p>
<pre>
# COCO Class names
# Index of the class in the list is its ID. For example, to get ID of
# the teddy bear class, use: class_names.index('teddy bear')
class_names = ['BG', 'person', 'bicycle', 'car', 'motorcycle', 'airplane',
               'bus', 'train', 'truck', 'boat', 'traffic light',
               'fire hydrant', 'stop sign', 'parking meter', 'bench', 'bird',
               'cat', 'dog', 'horse', 'sheep', 'cow', 'elephant', 'bear',
               'zebra', 'giraffe', 'backpack', 'umbrella', 'handbag', 'tie',
               'suitcase', 'frisbee', 'skis', 'snowboard', 'sports ball',
               'kite', 'baseball bat', 'baseball glove', 'skateboard',
               'surfboard', 'tennis racket', 'bottle', 'wine glass', 'cup',
               'fork', 'knife', 'spoon', 'bowl', 'banana', 'apple',
               'sandwich', 'orange', 'broccoli', 'carrot', 'hot dog', 'pizza',
               'donut', 'cake', 'chair', 'couch', 'potted plant', 'bed',
               'dining table', 'toilet', 'tv', 'laptop', 'mouse', 'remote',
               'keyboard', 'cell phone', 'microwave', 'oven', 'toaster',
               'sink', 'refrigerator', 'book', 'clock', 'vase', 'scissors',
               'teddy bear', 'hair drier', 'toothbrush']
</pre>
<p>However, Mask R-CNN runs through all of the classes, making it inefficient. Eventually, we hope to replace it with a smaller dataset focused on vehicles, which will make the processing faster. </p>
<p>The script loads a pre-trained model (line 44), collects the rgb_image from the camera, and then processes the image against the model, creating an array of detected objects (line 87):</p>
<pre>
# Load pre-trained model
model.load_weights(COCO_MODEL_PATH, by_name=True)
...
...
# Run the image through the Mask R-CNN model to get results.
results = model.detect([rgb_image], verbose=0)
</pre>
<p>We use the built-in visualize function to display the masks of these objects on the screen:</p>
<pre>
# Show the frame of video on the screen
mrcnn.visualize.display_instances(rgb_image, r['rois'], r['masks'], r['class_ids'], class_names, r['scores'])
</pre>
<div style="text-align: center;">
<img style="height: 480px" src="README_images/readme10.png" alt="Mask R-CNN masking and scoring">
<p style="margin-top: 0; font-style: italic; font-size: smaller;">Mask R-CNN masking and scoring</p>
</div>
<p>Once again, look at the code and note the changes between demo1.py and demo2.py, especially the imports. Also note that if the dataset you downloaded earlier is missing, the script will automatically download it from Matterport's GitHub repository, using mrcnn.utils on line 37. If you are behind a firewall, as we were at NASA, you can disabled this functionality with the following code:</p>
<pre>
# Do not download COCO trained weights from Releases if needed!
if not os.path.exists(COCO_MODEL_PATH):
    # mrcnn.utils.download_trained_weights(COCO_MODEL_PATH)
    print("Oh no! We are missing the mask_rcnn_coco.h5 dataset!")
</pre>
<hr>
<h3 id="d3">Demo 3 - Recognizing and Counting Vehicles</h3>
<p>In the previous demo, Mask R-CNN placed masks on every item it detected. However, as we stated earlier, we are only looking for vehicles. Therefore, we added the get_car_boxes function (line 25), which returns a numpy array of the cars (#3), trucks (#6), and buses (#8) that Mask R-CNN detected: </p>
<pre>
# Filter a list of Mask R-CNN detection results to get only the detected cars / trucks
def get_car_boxes(boxes, class_ids):
    car_boxes = []

    for i, box in enumerate(boxes):
        # If the detected object isn't a car / truck, skip it
        if class_ids[i] in [3, 8, 6]:
            car_boxes.append(box)

    return np.array(car_boxes)
</pre>
<p>We use that array with the OpenCV rectangle function to box the vehicles in (line 102):</p>
<pre>
# Draw the box
cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 255, 0), 2)
</pre>
<div style="text-align: center;">
<img style="height: 480px" src="README_images/readme11.png" alt="Identifying vehicles">
<p style="margin-top: 0; font-style: italic; font-size: smaller;">Identifying vehicles</p>
</div>
<p>We also acquired their locations in the frame, which we will use this later to determine if a vehicle is within a zone.</p>
<div style="text-align: center;">
<img style="height: 480px" src="README_images/readme11b.png" alt="Raw counts and coordinates">
<p style="margin-top: 0; font-style: italic; font-size: smaller;">Raw counts and coordinates</p>
</div>
<hr>
<h3 id="d4">Demo 4 - Creating and Displaying Zones</h3>
<p>Next, we create the zones using OpenCV. We will only count cars within these zones, to avoid counting cars on roads, other parking lots, etc. In addition, each of these zones will have a zone type (e.g., employee, handicap, etc.), which will allow us to provide better counts to the users.</p>
<p>Using the frame you captured in demo1.py and your favorite image viewer/editor, determine the pixel coordinates of each zone and enter them in the poly_coords array. For demo4.py, we created two zones, employee and handicap, and overlaid them on the frame:</p>
<pre>
# Read clockwise from top-left corner
poly_coords = ([[751, 1150], [3200, 1140], [3200, 1350], [851, 1400]],
                [[240, 1140], [750, 1150], [850, 1400], [150, 1400]])

\# BGR colors: Orange, Blue, Red, Gray, Yellow, Cyan, Pink, White
colors = [[0, 127, 255], [255, 0, 0], [0, 0, 255], [127, 127, 127],
            [0, 255, 255], [255, 255, 0], [127, 0, 255], [255, 255, 255]]

\# Make an overlay for transparent boxes
overlay = frame.copy()

\# Draw the filled zones
for index, p in enumerate(poly_coords, start=0):
    cv2.fillPoly(overlay, np.int32(
        [np.array(p)]), colors[index + 4])

\# Set transparency for boxes
alpha = 0.4
\# Add overlay to frame
frame = cv2.addWeighted(overlay, alpha, frame, 1 - alpha, 0)

\# Optional Draw the zone boundaries
for index, p in enumerate(poly_coords, start=0):
    cv2.polylines(frame, np.int32(
        [np.array(p)]), True, colors[index], 10)

\# Draw center crosshair
height, width, channels = frame.shape
cv2.drawMarker(frame, (int(width / 2), int(height / 2)),
                [255, 255, 0], cv2.MARKER_TRIANGLE_UP, 16, 2, cv2.LINE_4)
\# Add timestamp
cv2.putText(frame, timestamp, (10, 30),
            cv2.FONT_HERSHEY_SIMPLEX, 1, [0, 0, 255], 1)
</pre>
<div style="text-align: center;">
<img style="height: 480px" src="README_images/readme12.png" alt="Creating and displaying zones">
<p style="margin-top: 0; font-style: italic; font-size: smaller;">Creating and displaying zones</p>
</div>
<p>The cyan triangle in the middle is a calibration point. For Pan-Tilt-Zoom (PTZ) cameras, if the triangle and an outside reference point (e.g., a cone, sign, etc.) line up, the zone overlays are placed correctly on the frame. To see your work, this script also saves a time-stamped scaled copy of the image in the demo_captures folder, named "[YYMMDD]_d4_capture.jpg".</p>
<hr>
<h3 id="d5">Demo 5 - Counting Vehicles in Zones</h3>
<p>Now that we have our zones, we will run Mask R-CNN again, but we will use Shapely's centroid and intersects function to determine if a vehicle is in a zone (line 127). Using the centerpoint prevents vehicles from being counted in a zone they overlap, but do not occupy. If a vehicle is in a zone, it is added to the zone's count (line 132). However, is is deleted from the numpy array, to avoid double counting cars that may intersect more than one zone (line 134):</p>
<pre>
# Only show cars in the zones!
if(((Polygon([(x1, y1), (x2, y1), (x1, y2), (x2, y2)])).centroid).intersects(Polygon(asPoint(array(p))))):
    # Draw the box and add to overlay
    cv2.rectangle(frame, (x1, y1), (x2, y2),
                    colors[index], 5)
    # Count car in zone
    count += 1
    # Delete the car to avoid double counting
    np.delete(car_boxes, box)
</pre>
<div style="text-align: center;">
<img style="height: 480px" src="README_images/readme13.png" alt="Identifying vehicles within zones">
<p style="margin-top: 0; font-style: italic; font-size: smaller;">Identifying vehicles within zones</p>
</div>
<div style="text-align: center;">
<img style="height: 480px" src="README_images/readme14.png" alt="Counting vehicles within zones">
<p style="margin-top: 0; font-style: italic; font-size: smaller;">Counting vehicles within zones</p>
</div>
<hr>
<h3 id="d6">Demo 6 - Creating the Database</h3>
<p>Like we stated earlier, due to architecture, security requirements, cross-lot tracking, etc., the NASA application was complex, and required two separate databases. For our demo, we will use a simpler data model, and, for portabililty (and to keep this README from being longer than it already is!) we'll use SQLite to hold the data.</p>
<p>As stated earlier, Park counts the number of vehicles in parking zones. We found that a single camera cannot always observe a complete lot. Instead, a camera can observe portions of multiple lots and a lot can be observed by multiple cameras. Therefore, we divided each feed into zones, with each zone associated with a lot, and, as shown in demo4.py, each zone is classified as employee parking, handicap parking, etc. In demo7.py. we will show you how Park cycles through and analyzes all the feeds every five minutes, collecting the counts for the zones that each camera can observe. Once the cycle is complete, Park can aggregate the counts into lot totals, which the front end can present to the user.</p>
<p>Here's the data model:</p>
<div style="text-align: center;">
<img style="height: 480px" src="README_images/readme15.png" alt="Park Data Model">
<p style="margin-top: 0; font-style: italic; font-size: smaller;">Park Data Model </p>
</div>
<p>And here are the table descriptions:</p>
<ul>
<li><b>Source:</b> This table contains a list of the sources, such as cameras, that collect images, as well as their location and credentials. The camera's URL is a unique value.</li>
<li><b>Lot:</b> This table contains a list of parking lots, as well as their location in latitude and longitude. The lot name is a unique value.</li>
<li><b>Type:</b> This table contains a list of zone types, such as employee parking, handicap parking, etc. The description is a unique value.</li>
<li><b>Zone:</b> This table contains all the zones from all the lots in the database. It also contains the total number of parking spaces within each zone, as well as the boundaries of the zone within its source's frame. This table has a one-to-many relationship with Source, Lot, and Type, and a unique compound key comprised of the primary keys of each of those tables and its own ZoneID.</li>
<li><b>OccupancyLog:</b> This table is a junction table (i.e., an associative entity) that collects and timestamps all the zone counts, providing both current and historical parking data. This table has a one-to-many relationship with Lot, Zone, and Type, and a unique compound key comprised of the primary keys of each of those tables and its own Timestamp.</li>
</ul>
<p>Run demo6.py to create the database in the db folder and open the database using the following commands:</p>
<pre>
[park@localhost Park]$ ./demo6.py
[park@localhost Park]$ ls db
[park@localhost Park]$ sqlite3 db/park.db
</pre>
<div style="text-align: center;">
<img style="height: 480px" src="README_images/readme16.png" alt="Creating the Database">
<p style="margin-top: 0; font-style: italic; font-size: smaller;">Creating the Database </p>
</div>
<p>Once complete, test the new database with the some queries. Here are some examples; if they do not work, check for any errors that may have occurred when you created the database:</p>
<pre>
SELECT * FROM Lot ORDER BY Name ASC;
SELECT * FROM Lot WHERE LotID = 1;
SELECT * FROM Type ORDER BY TypeID ASC;
SELECT * FROM Source ORDER BY SourceID ASC;
SELECT Zone.ZoneID, Source.Location, Type.Description, Lot.Name, Zone.TotalSpaces, Zone.PolyCoords
FROM Zone
INNER JOIN Source ON Zone.SourceID = Source.SourceID
INNER JOIN Type ON Zone.TypeID = Type.TypeID
INNER JOIN Lot ON Zone.LotID = Lot.LotID
ORDER BY Lot.Name, Type.Description;
</pre>
<div style="text-align: center;">
<img style="height: 480px" src="README_images/readme17.png" alt="Running test queries">
<p style="margin-top: 0; font-style: italic; font-size: smaller;">Running test queries </p>
</div>
<hr>
<h3 id="d7">Demo 7 - Putting It All Together.</h3>
<div style="text-align: center;">
<img src="README_images/park_demo.gif" alt="Putting it all together">
<p style="margin-top: 0; font-style: italic; font-size: smaller;">Putting it all together</p>
</div>
<p>Finally! demo7.py is a super demo. It incorporates all of the functionality of the previous demos, and actually connects to the database to get feed credentials and to update data.</p>
<div style="background-color: lightgoldenrodyellow; padding: 6px;">
<p>
<strong>By the way, demo7.py requires user interaction after displaying each frame. The above image is an animation of the demo7.py results, created using GIMP.</strong>
</p>
</div>
<hr>
<div style="width: 100%; text-align: center;">
<img src="sherpa_logo.png" alt="Sherpa Logo" style="height:240px;" />
</div>
<p style="text-align: center;">The Sherpas are Jrei Dimanno, Justine Forrest, Rob Garcia, John Graham, Tanner Griffin, Ryan Hashi, Brandon Hutton, Gabriel Jacobs, Fernando Jauregui, Wesley Madden, and Doug Trent</p>
<hr>
<h2 id="p10">References</h2>
<p>Geitgey, A. (2019, January 21). Snagging parking spaces with Mask R-CNN and Python. Retrieved from <a href="https://medium.com/@ageitgey/snagging-parking-spaces-with-mask-r-cnnand-python-955f2231c400" target="_blank" title="Snagging parking spaces with Mask R-CNN and Python">https://medium.com/@ageitgey/snagging-parking-spaces-with-mask-r-cnnand-python-955f2231c400 </p>
</a>
<p>INRIX. (2017, July 12). Searching for parking costs Americans $73 billion a year. Retrieved from <a href="http://inrix.com/press-releases/parking-pain-us/" target="_blank" title="Searching for parking costs Americans $73 billion a year">http://inrix.com/press-releases/parking-pain-us/ </p>
</a>
<p>International Parking and Mobility Institute. (2015, October 12). Open requests for proposals. Retrieved July 31, 2019, from <a href="https://www.parking.org/membership/member-resources/rfps/" target="_blank" title="Open requests for proposals">https://www.parking.org/membership/member-resources/rfps/ </p>
</a>
<p>Matterport, Inc. (2019, March 10). matterport/Mask_RCNN. Retrieved August 2, 2019, from <a href="https://github.com/matterport/Mask_RCNN" target="_blank" title="matterport/Mask_RCNN">https://github.com/matterport/Mask_RCNN</a>
</p>